"Digital Service Review - Alpha Report"

"Standard","Responsibilities","Decision (Met, Partly met, Did not meet)","What the team has done well","What the team needs to explore"
"Design with users","Multiple ideas and/or prototypes have been tested. One solution has been selected for Beta development.   To meet this standard, you usually need to:   Research with current and/or potential users with varying needs and diverse backgrounds using quantitative and qualitative dataUnderstand how users interact with the service, optimizing the experience for online and offline interactionsDo user research with prototypesDemonstrate how research findings influence decision-making and improve the design of the productTest hypotheses against the problem statementInclude all team members in user research (participating, observing or reviewing research findings)","Met
Partly met
Did not meet","The panel was impressed that the team: Actual users of the service tested the service.Included diverse demographics, languages, and device criteria for user research participation.","Before their next review, the team needs to: Ensure more employees of the product team are active participants in user research (observing is good, taking notes is better).Recommend that the department build sustainable methods of doing user research that are not tied to other organizations."
"Iterate and improve frequently","Share the evolution of testing hypotheses and prototypes.   To meet this standard, you usually need to:   Ensure team can change prototype development given new research or feedbackEstablish agile practices, like team retrospectives, reviews, showcases, product backlog, etc.Explain how user research and feedback reflects the team's design decisionsDevelop a plan for tracking key performance metrics to measure the success of the prototypeExplain processes for continuously seeking and prioritising feedbackEnsure access to environments and resources to enable rapid prototyping","Met
Partly met
Did not meet","The panel was impressed that the team: Incorporated feedback from user testing in new iterations.Has automated testing in place: unit, CodeQL, synk, limited e2e tests and GitHub is being used for package monitoring.Production and non-production environments are very similar.","Before their next review, the team needs to: Start building and sharing a coded prototype sooner rather than later. This makes it easier to test with users who use assistive technology easier and let's your partners actively test your service.Show more metrics around iterations (number of releases, changes, iterations, etc.).Monitor development metrics (e.g. mean time to restore, deployment frequency, change failure rate, change lead time, etc.)Identify service level metrics."
"Work in the open by default","Share prototypes and user research findings with partners and stakeholders.   To meet this standard, you usually need to:   Scan for services offered to the public to identify similar work to learn from and build onDocument and share the team’s work including research consulted, research plans, prototypes and codeShare research findings and evidence to support design decisionsShowcase problem statements, prototypes, etc. to partners and stakeholdersShare open status updates for peers who are interested in the workDraft an internal/external communications plan","Met
Partly met
Did not meet","The panel was impressed that the team: Shared research findings across the organization.","Before their next review, the team needs to: Find ways to share more outside of DECD and ESDCFind ways to share more outside of government, like civic tech communitiesIf you cannot share your work publicly yet, share your work in spaces where you'll find the communities you're looking for that's more private (like GCTools, GCxchange, DevOps Slack space, Web Communications Slack space, etc.)"
"Use open standards and solutions","Evaluate existing services and choose the most appropriate option to develop the MVP.   To meet this standard, you usually need to:   Determine whether you need to use an alternative platform or technologyDevelop a plan to test compatibility across platforms and devicesConsider how to integrate the service with legacy systemsUse design system components for prototypingUnderstand what third-party APIs the service will need to integrate withConsider how an existing solution might be altered to meet user needs","Met
Partly met
Did not meet","The panel was impressed that the team: Considered front to back service integration over the long-term.","Before their next review, the team needs to: Ensure that open standards are used. Where necessary, refer to Government of Canada Standards, like the Standards on APIsPush back on organizational standards that are closed. Ask why they are closed and reference the Policy on Service and Digital."
"Address security and privacy risks","Develop a plan to understand potential threats and how to address them.   To meet this standard, you usually need to:   Ensure that user research participants understand what information they are sharing and provide informed consentPrepare for any required privacy assessmentsEngage with subject matter experts to help develop security and privacy protection plansDocument how the service manages information and records to ensure confidentiality, integrity, and availability of the dataConsider relevant legislation and risk associated with the data required to design and support the serviceIdentify possible threats to your service and start thinking about ways to reduce them","Met
Partly met
Did not meet","The panel was impressed that the team: Team has a dashboard to monitor service errors.Design, specifically the header, was created with prominent sign out button for users to that they are in a sign-in state.Has automated testing: unit, CodeQL, synk, limited e2e tests and GitHub is being used for package monitoring.","Before their next review, the team needs to: Better illustrate how security and privacy risks are assessed.Start content design and policy work on Privacy Statement and Security Statement for users and test them.Start thinking about threat modelling for your systems.Ensure that there's a short feedback loop on the service so the team knows when things break."
"Build in accessibility from the start","Test prototypes with a variety of users with accessibility needs.   To meet this standard, you usually need to:   Understand how to make services accessible (refer to Canadian Human Rights Act, Standard on Web Accessibility, WCAG, Accessible Canada Act, etc.)Consider the lowest levels of digital skill, confidence and access when designing prototypesConduct ongoing research with users with low-level digital literacy, people with disabilities, and people from different cultural and linguistic backgroundsEngage with accessibility experts to get feedback on prototypesWhen possible, design your service to be inclusive of those who can't access the digital channel for various reasons (consider the digital divide and adoption rates among diverse communities)Consider non-digital channels and be able to speak to how and why those are being supported or notAt a minimum, ensure prototypes meet WCAG 2.0 conformance requirements","Met
Partly met
Did not meet","The panel was impressed that the team: Screening criteria for user research included recent immigrations and people who identify as having visual or cognitive challenges.Designs included options for users to get support from other channels.Consulted with accessibility specialists for guidance.Designed to meet WCAG AA standards.Services built using framework that's ideal for mobile-first designs.Performed accessibility tests per component.","Before their next review, the team needs to: Share both screening criteria for user research and who was actually recruited for testing.Do design research with people with disabilities starting in Discovery phase and continue it in every phase. It's beneficial to surface A11y needs beyond compliance to provide a dignified user experience.Build a coded prototype sooner than later to test with users who use assistive technology."
"Empower staff to deliver better services","Team has access to environments to experiment with new approaches, tooling or solutions.   To meet this standard, you usually need to:   Work with developers to determine technical choices and programming tools to develop Beta BuildCheck for risks or restrictions associated with the tools and avoid any contracts that will prevent you from changing/improving your serviceEnsure all members of the team are actively weighing in on prototype design, regardless of their disciplineEnsure team members can articulate how their work contributes to the organization's outcomes","Met
Partly met
Did not meet","The panel was impressed that the team: N/A","Before their next review, the team needs to: The standard was not covered in the demo and Q&A so it could not be reviewed.Next time, the team can speak to how they work, their process enablers or blockers, and what tools they use"
"Be good data stewards","The team knows what data will be collected and the standards that apply for using it.   To meet this standard, you usually need to:   Use open data for researchPrioritize what data should be collected and validate the need for each data element in scopeJustify why personal data needs to be collected to use the serviceTell users where and how their data will be usedEngage with data experts to develop a plan for protecting dataIdentify and remove barriers to data sharing and releaseDocument data flowsCollect and analyze baseline data (by channel) to inform prototype development","Met
Partly met
Did not meet","The panel was impressed that the team: N/A","Before their next review, the team needs to: The standard was not covered in the demo and Q&A so it could not be reviewed.Next time, the team can speak to data from service design and user research work."
"Design ethical services","Identify indicators of bias and ways to eliminate unintended consequences to the user when testing prototypes.   To meet this standard, you usually need to:   Identify ways to provide additional support to the users who need itRecruit a wide range of users for testingBe aware of and seek out indicators of bias when selecting people for user researchDevelop a plan to acknowledge, account for and mitigate possible biases in the data and algorithms for automated decision-makingConsider how users will be able to appeal automated decisions","Met
Partly met
Did not meet","The panel was impressed that the team: Screening criteria for user research included recent immigrations and people who identify as having visual or cognitive challenges.Designs included options for users to get support from other channels.","Before their next review, the team needs to: Show a snapshot of who was recruited and how those users informed design requirements and decisions.Seek advice from policy advisors or recruit them to be a member of your team to help analyze your service from an ethics perspectiveShare both screening criteria for user research and who was actually recruited for testing"
"Collaborate widely","Collaborate with stakeholders that have built similar services. Find ways to reuse existing solutions.   To meet this standard, you usually need to:   Consult and collaborate with stakeholders who work on other channels (in-person channel, phone channel, etc.)Establish partnerships with peers, users, and stakeholders across the department, GC and/or other levels of government that are building similar servicesEngage with front line employees and gather user feedback on existing solutionsJoin communities of practice, events or groups that pertain to the areas of expertise of team membersEnsure the purpose of any collaboration (scope of work, timelines, accountability, etc.) is clearDevelop a plan to transfer knowledge and skills to new team members (or from contractors to permanent staff)Reuse testing tools, technologies and guidance that other teams have used (user research interview templates, data patterns, etc.)Link to the work of others","Met
Partly met
Did not meet","The panel was impressed that the team: The team consulted, collaborated, and built partnerships with teams across DECD and ESDC.","Before their next review, the team needs to: Connect with service teams outside of ESDC who may have faced similar challenges. Try finding them on GCxchange, Twitter, or various Slack spaces (like GCTools, GCxchange, DevOps Slack space, Web Communications Slack space, etc.)"
" Assessment date: June 21, 2022  Result: Pass  Stage: Alpha"
"Alpha Review"
"Secure Client Hub Team"
"Service description"
"Service users"
"We are prototyping the landing page after signing in to My Service Canada Account (MSCA) where users can see and manage their existing benefits regardless of which software application handles the benefit (legacy or new tool, Curam).   Users will have quick access to the most relevant information for their pending, active or past benefits. By developing a replacement for the MSCA Lobby and Program pages, users can easily identify and complete tasks relevant to them and their current situation with an improved user experience."
"People returning to My Service Canada Account manage their benefitPeople helping others manage benefits in My Service Canada Account"
"What the service team is doing well "
"Kudos to the team! The Standards are hard to meet and it's great to see so much good work and effort."
"What the service team should explore next"
"What are the service's privacy and security considerations, and what are the user needs related to them?How might the team provide a dignified user experience for users accessing government benefits?How might we measure success of the service? How might we measure the client experience?How might the team do targeted testing with specific user groups to identify diverse needs and barriers to adoption?How might the team decouple deployments of systems with releases (e.g. ability to turn on and off features so it's not tied to the tech process)?What does an organized, up to date, public, roadmap for your stakeholders (so they can largely self serve) look like?What should be the team's refined backlog for Beta?How might we shorten feedback loops as much as possible, so the team and partners are reviewing the most updated service?What does it mean to treat Beta Build as production to build a more robust system going forward?Who are the champions (executive level or otherwise) you need for Beta Build?"













